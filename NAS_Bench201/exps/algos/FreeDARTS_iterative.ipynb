{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys, time, glob, random, argparse\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "sys.path.append(str('/home/cwang4/Data/Projects/AngleNAS-miao/NAS_Bench201/lib'))\n",
    "sys.path.append(str('/home/cwang4/Data/Projects/AngleNAS-miao/NAS_Bench201/configs'))\n",
    "from config_utils import load_config, dict2config, configure2str\n",
    "from datasets     import get_datasets, SearchDataset\n",
    "from procedures   import prepare_seed, prepare_logger, save_checkpoint, copy_checkpoint, get_optim_scheduler\n",
    "from utils        import get_model_infos, obtain_accuracy\n",
    "from log_utils    import AverageMeter, time_string, convert_secs2time\n",
    "from models       import get_cell_based_tiny_net, get_search_spaces\n",
    "from nas_102_api  import NASBench102API as API\n",
    "import torch.nn.functional as F\n",
    "from weight_initializers import init_net\n",
    "\n",
    "\n",
    "def pruning_func(xloader, network, criterion, scheduler, w_optimizer, a_optimizer, print_freq, logger):\n",
    "    data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "    base_losses, base_top1, base_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    arch_losses, arch_top1, arch_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    network.train()\n",
    "    end = time.time()   \n",
    "      \n",
    "\n",
    "    for name, param in network.module.state_dict().items():\n",
    "        if name=='arch_parameters':###whether abs on arch parameter\n",
    "            print(param)\n",
    "            param.abs_()\n",
    "        else:\n",
    "            param.abs_()\n",
    "    arch_pruned=deepcopy(network.module.arch_parameters.data)\n",
    "    \n",
    "    \n",
    "    for step, (base_inputs, base_targets, arch_inputs, arch_targets) in enumerate(xloader):\n",
    "        scheduler.update(None, 1.0 * step / len(xloader))\n",
    "\n",
    "        base_targets=torch.tensor(base_targets).to(torch.float32)\n",
    "        arch_targets=torch.tensor(arch_targets).to(torch.float32)\n",
    "        # print(base_inputs)\n",
    "        # print(base_targets)\n",
    "        print(args.labelrandom)\n",
    "        if args.labelrandom=='random':\n",
    "            base_targets=torch.randn_like(base_targets).cuda()\n",
    "            arch_targets=torch.randn_like(arch_targets).cuda()\n",
    "        elif args.labelrandom=='allone':\n",
    "            base_targets=torch.ones_like(base_targets).cuda()\n",
    "            arch_targets=torch.ones_like(arch_targets).cuda()\n",
    "            # arch_inputs=torch.ones_like(arch_inputs).cuda()            \n",
    "        elif args.labelrandom=='data':\n",
    "            base_targets = base_targets.cuda(non_blocking=True)\n",
    "            arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "        else:\n",
    "            NotImplementedError\n",
    "        # measure data loading time\n",
    "\n",
    "        # update the architecture-weight\n",
    "        a_optimizer.zero_grad()\n",
    "        \n",
    "        input_dim = list(arch_inputs[0,:].shape)\n",
    "        # inputs = torch.ones([1] + input_dim).float().cuda(non_blocking=True)\n",
    "        if args.datarandom=='random':\n",
    "            inputs=torch.randn_like(base_inputs).cuda()\n",
    "            # arch_inputs=torch.randn_like(arch_inputs).cuda()\n",
    "        elif args.datarandom=='allone':\n",
    "            inputs=torch.ones_like(base_inputs).cuda()\n",
    "            # arch_inputs=torch.ones_like(arch_inputs).cuda()            \n",
    "        elif args.datarandom=='data':\n",
    "            inputs=base_inputs\n",
    "        else:\n",
    "            NotImplementedError\n",
    "            \n",
    "        # print(inputs)\n",
    "        _, logits = network.forward(inputs)        \n",
    "        \n",
    "        arch_loss = torch.sum(logits)\n",
    "        arch_loss.backward()\n",
    "        #a_optimizer.step()\n",
    "        #arch_synflow=abs(F.softmax(network.module.arch_parameters.data)*network.module.arch_parameters.grad.data)\n",
    "        #arch_synflow=abs(network.module.arch_parameters.grad.data)        \n",
    "        # arch_synflow=abs(network.module.arch_parameters.data*network.module.arch_parameters.grad.data)        \n",
    "        # print(network.module.arch_parameters.data)\n",
    "        # print(network.module.arch_parameters.grad.data)   \n",
    "        #####################################################################################\n",
    "        #break\n",
    "        num_batchs=1\n",
    "        if step%num_batchs==0:         \n",
    "            synflow=np.zeros(5)            \n",
    "            availabe_edge=[]\n",
    "            for edge in range(6):\n",
    "                num_of_oper=0\n",
    "                for i in range(5):\n",
    "                    if arch_pruned[edge,i]> -float('Inf'):\n",
    "                        num_of_oper +=1\n",
    "                if num_of_oper>1:\n",
    "                    availabe_edge.append(edge)\n",
    "            edge_ind= np.random.randint(len(availabe_edge))   \n",
    "            edge=availabe_edge[edge_ind]\n",
    "            \n",
    "            num_of_oper=0\n",
    "            for i in range(5):\n",
    "                if arch_pruned[edge,i]> -float('Inf'):\n",
    "                    # snip[i]=abs(network.module.arch_parameters.data[edge,i]*network.module.arch_parameters.grad.data[edge,i])\n",
    "                    synflow[i]= network.module.arch_parameters.data[edge,i]*network.module.arch_parameters.grad.data[edge,i]\n",
    "                    num_of_oper +=1\n",
    "                else:\n",
    "                    synflow[i]= float('Inf')\n",
    "        else:\n",
    "            synflow_new=np.zeros(5)\n",
    "            for i in range(5):\n",
    "                if arch_pruned[edge,i]> -float('Inf'):\n",
    "                    synflow_new[i]= network.module.arch_parameters.data[edge,i]*network.module.arch_parameters.grad.data[edge,i]\n",
    "                    # snip_new[i]=abs(network.module.arch_parameters.data[edge,i]*network.module.arch_parameters.grad.data[edge,i])\n",
    "\n",
    "            synflow = synflow + synflow_new    \n",
    "                    \n",
    "        if (step+1)%num_batchs==0:      \n",
    "            if num_of_oper>1:\n",
    "                ind=np.argmin(synflow)\n",
    "                arch_pruned[edge,ind]=-float('Inf')\n",
    "            logger.log('{:}'.format(api.query_by_arch(network.module.genotype_prune(arch_pruned))))  \n",
    "            total_ope=0\n",
    "            for edge in range(6):\n",
    "                for i in range(5):\n",
    "                    if arch_pruned[edge,i]> -float('Inf'):\n",
    "                        total_ope +=1\n",
    "            print(arch_pruned)\n",
    "            print('---------------------------------------------')                                                \n",
    "            if total_ope==6:\n",
    "                break\n",
    "    \n",
    "    print('kaishi-----------------------------------')  \n",
    "            \n",
    "    return arch_pruned\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\"DARTS First Order\")\n",
    "parser.add_argument('--data_path',          type=str,   default= '/home/cwang4/Data/Datasets', help='Path to dataset')\n",
    "parser.add_argument('--dataset',            type=str,   default= 'cifar10', choices=['cifar10', 'cifar100', 'ImageNet16-120'], help='Choose between Cifar10/100 and ImageNet-16.')\n",
    "# channels and number-of-cells\n",
    "parser.add_argument('--search_space_name',  type=str,   default= 'nas-bench-102',help='The search space name.')\n",
    "parser.add_argument('--max_nodes',          type=int,   default= 4 ,help='The maximum number of nodes.')\n",
    "parser.add_argument('--channel',            type=int,   default= 16,help='The number of channels.')\n",
    "parser.add_argument('--num_cells',          type=int,   default= 5, help='The number of cells in one stage.')\n",
    "# architecture leraning rate\n",
    "parser.add_argument('--arch_learning_rate', type=float, default=3e-2, help='learning rate for arch encoding')\n",
    "parser.add_argument('--arch_weight_decay',  type=float, default=1e-3, help='weight decay for arch encoding')\n",
    "# log\n",
    "parser.add_argument('--workers',            type=int,   default=2,    help='number of data loading workers (default: 2)')\n",
    "parser.add_argument('--save_dir',           type=str,   default='/output/search-cell-nas-bench-102-cifar10',help='Folder to save checkpoints and log.')\n",
    "parser.add_argument('--arch_nas_dataset',   type=str,   default='/home/cwang4/Data/Projects/AngleNAS-miao/NAS_Bench201/NAS-Bench-201-v1_0-e61699.pth',help='The path to load the architecture dataset (tiny-nas-benchmark).')\n",
    "parser.add_argument('--print_freq',         type=int,   default=50,help='print frequency (default: 200)')\n",
    "parser.add_argument('--rand_seed',          type=int,   default=327, help='manual seed')\n",
    "parser.add_argument('--datarandom',            type=str,   default= 'data', choices=['data', 'random', 'allone'], help='.')\n",
    "parser.add_argument('--labelrandom',            type=str,   default= 'random', choices=['data', 'random', 'allone'], help='.')\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Function with logger : Logger(dir=00FreeDARTS_SynFlow_seed_327/output/search-cell-nas-bench-102-cifar10, use-tf=False, writer=None)\n",
      "Arguments : -------------------------------\n",
      "arch_learning_rate : 0.03\n",
      "arch_nas_dataset : /home/cwang4/Data/Projects/AngleNAS-miao/NAS_Bench201/NAS-Bench-201-v1_0-e61699.pth\n",
      "arch_weight_decay : 0.001\n",
      "channel          : 16\n",
      "data_path        : /home/cwang4/Data/Datasets\n",
      "datarandom       : data\n",
      "dataset          : cifar10\n",
      "labelrandom      : random\n",
      "max_nodes        : 4\n",
      "num_cells        : 5\n",
      "print_freq       : 50\n",
      "rand_seed        : 327\n",
      "save_dir         : ./00FreeDARTS_SynFlow_seed_327/output/search-cell-nas-bench-102-cifar10\n",
      "search_space_name : nas-bench-102\n",
      "workers          : 2\n",
      "Python  Version  : 3.8.16 (default, Mar  2 2023, 03:21:46)  [GCC 11.2.0]\n",
      "Pillow  Version  : 9.4.0\n",
      "PyTorch Version  : 1.12.1+cu113\n",
      "cuDNN   Version  : 8302\n",
      "CUDA available   : True\n",
      "CUDA GPU numbers : 1\n",
      "CUDA_VISIBLE_DEVICES : None\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Load split file from /home/cwang4/Data/Projects/AngleNAS-miao/NAS_Bench201/configs/nas-benchmark/cifar-split.txt\n",
      "/home/cwang4/Data/Projects/AngleNAS-miao/NAS_Bench201/configs/nas-benchmark/algos/DARTS.config\n",
      "Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=50, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, class_num=10, xshape=(1, 3, 32, 32))\n",
      "||||||| cifar10    ||||||| Search-Loader-Num=391, Valid-Loader-Num=391, batch size=64\n",
      "||||||| cifar10    ||||||| Config=Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=50, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, class_num=10, xshape=(1, 3, 32, 32))\n",
      "w-optimizer : SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    initial_lr: 0.025\n",
      "    lr: 0.025\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: True\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "a-optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.5, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.03\n",
      "    maximize: False\n",
      "    weight_decay: 0.001\n",
      ")\n",
      "w-scheduler : CosineAnnealingLR(warmup=0, max-epoch=50, current::epoch=0, iter=0.00, type=cosine, T-max=50, eta-min=0.001)\n",
      "criterion   : CrossEntropyLoss()\n",
      "FLOP = 243.71 M, Params = 1.69 MB\n",
      "try to create the NAS-Bench-102 api from /home/cwang4/Data/Projects/AngleNAS-miao/NAS_Bench201/NAS-Bench-201-v1_0-e61699.pth\n",
      "[2023-04-25 11:48:31] create API = NASBench102API(15625/15625 architectures) done\n",
      "tensor([[ 5.1758e-04,  1.7865e-03, -2.4616e-04,  6.3276e-04, -6.3033e-04],\n",
      "        [ 6.3756e-04,  4.6908e-04, -9.2583e-04, -1.5301e-03, -1.0401e-04],\n",
      "        [-2.9429e-04,  8.8370e-04,  7.1474e-04,  5.4000e-04, -1.2266e-04],\n",
      "        [ 1.1660e-03, -1.9962e-03, -9.0506e-04, -8.8039e-04,  9.5113e-05],\n",
      "        [ 9.3590e-04, -1.9302e-03, -9.0115e-04, -1.6130e-03, -1.0693e-03],\n",
      "        [ 3.5560e-04,  1.6399e-05,  4.5186e-04,  7.4616e-06,  1.1256e-03]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15299/761240300.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  base_targets=torch.tensor(base_targets).to(torch.float32)\n",
      "/tmp/ipykernel_15299/761240300.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  arch_targets=torch.tensor(arch_targets).to(torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random\n",
      "|skip_connect~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|skip_connect~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=10032\n",
      "cifar10-valid  FLOP= 43.17 M, Params=0.316 MB, latency=12.08 ms.\n",
      "cifar10-valid  train : [loss = 0.645, top1 = 77.35%], valid : [loss = 0.748, top1 = 74.42%]\n",
      "cifar10        FLOP= 43.17 M, Params=0.316 MB, latency=11.82 ms.\n",
      "cifar10        train : [loss = 0.522, top1 = 82.05%], test  : [loss = 0.573, top1 = 80.53%]\n",
      "cifar100       FLOP= 43.18 M, Params=0.322 MB, latency=11.76 ms.\n",
      "cifar100       train : [loss = 2.000, top1 = 46.39%], valid : [loss = 2.102, top1 = 44.17%], test : [loss = 2.100, top1 = 44.35%]\n",
      "ImageNet16-120 FLOP= 10.80 M, Params=0.323 MB, latency=9.51 ms.\n",
      "ImageNet16-120 train : [loss = 3.169, top1 = 23.59%], valid : [loss = 3.123, top1 = 24.93%], test : [loss = 3.175, top1 = 23.83%]\n",
      "tensor([[5.1758e-04, 1.7865e-03, 2.4616e-04, 6.3276e-04, 6.3033e-04],\n",
      "        [6.3756e-04, 4.6908e-04, 9.2583e-04, 1.5301e-03, 1.0401e-04],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04, 5.4000e-04, 1.2266e-04],\n",
      "        [      -inf, 1.9962e-03, 9.0506e-04, 8.8039e-04, 9.5113e-05],\n",
      "        [9.3590e-04, 1.9302e-03, 9.0115e-04, 1.6130e-03, 1.0693e-03],\n",
      "        [3.5560e-04, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|skip_connect~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|skip_connect~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=10032\n",
      "cifar10-valid  FLOP= 43.17 M, Params=0.316 MB, latency=12.08 ms.\n",
      "cifar10-valid  train : [loss = 0.645, top1 = 77.35%], valid : [loss = 0.748, top1 = 74.42%]\n",
      "cifar10        FLOP= 43.17 M, Params=0.316 MB, latency=11.82 ms.\n",
      "cifar10        train : [loss = 0.522, top1 = 82.05%], test  : [loss = 0.573, top1 = 80.53%]\n",
      "cifar100       FLOP= 43.18 M, Params=0.322 MB, latency=11.76 ms.\n",
      "cifar100       train : [loss = 2.000, top1 = 46.39%], valid : [loss = 2.102, top1 = 44.17%], test : [loss = 2.100, top1 = 44.35%]\n",
      "ImageNet16-120 FLOP= 10.80 M, Params=0.323 MB, latency=9.51 ms.\n",
      "ImageNet16-120 train : [loss = 3.169, top1 = 23.59%], valid : [loss = 3.123, top1 = 24.93%], test : [loss = 3.175, top1 = 23.83%]\n",
      "tensor([[5.1758e-04, 1.7865e-03, 2.4616e-04, 6.3276e-04, 6.3033e-04],\n",
      "        [      -inf, 4.6908e-04, 9.2583e-04, 1.5301e-03, 1.0401e-04],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04, 5.4000e-04, 1.2266e-04],\n",
      "        [      -inf, 1.9962e-03, 9.0506e-04, 8.8039e-04, 9.5113e-05],\n",
      "        [9.3590e-04, 1.9302e-03, 9.0115e-04, 1.6130e-03, 1.0693e-03],\n",
      "        [3.5560e-04, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|skip_connect~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|skip_connect~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=10032\n",
      "cifar10-valid  FLOP= 43.17 M, Params=0.316 MB, latency=12.08 ms.\n",
      "cifar10-valid  train : [loss = 0.645, top1 = 77.35%], valid : [loss = 0.748, top1 = 74.42%]\n",
      "cifar10        FLOP= 43.17 M, Params=0.316 MB, latency=11.82 ms.\n",
      "cifar10        train : [loss = 0.522, top1 = 82.05%], test  : [loss = 0.573, top1 = 80.53%]\n",
      "cifar100       FLOP= 43.18 M, Params=0.322 MB, latency=11.76 ms.\n",
      "cifar100       train : [loss = 2.000, top1 = 46.39%], valid : [loss = 2.102, top1 = 44.17%], test : [loss = 2.100, top1 = 44.35%]\n",
      "ImageNet16-120 FLOP= 10.80 M, Params=0.323 MB, latency=9.51 ms.\n",
      "ImageNet16-120 train : [loss = 3.169, top1 = 23.59%], valid : [loss = 3.123, top1 = 24.93%], test : [loss = 3.175, top1 = 23.83%]\n",
      "tensor([[5.1758e-04, 1.7865e-03, 2.4616e-04, 6.3276e-04, 6.3033e-04],\n",
      "        [      -inf, 4.6908e-04, 9.2583e-04, 1.5301e-03, 1.0401e-04],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04, 5.4000e-04, 1.2266e-04],\n",
      "        [      -inf, 1.9962e-03, 9.0506e-04, 8.8039e-04, 9.5113e-05],\n",
      "        [      -inf, 1.9302e-03, 9.0115e-04, 1.6130e-03, 1.0693e-03],\n",
      "        [3.5560e-04, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|skip_connect~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|skip_connect~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=10032\n",
      "cifar10-valid  FLOP= 43.17 M, Params=0.316 MB, latency=12.08 ms.\n",
      "cifar10-valid  train : [loss = 0.645, top1 = 77.35%], valid : [loss = 0.748, top1 = 74.42%]\n",
      "cifar10        FLOP= 43.17 M, Params=0.316 MB, latency=11.82 ms.\n",
      "cifar10        train : [loss = 0.522, top1 = 82.05%], test  : [loss = 0.573, top1 = 80.53%]\n",
      "cifar100       FLOP= 43.18 M, Params=0.322 MB, latency=11.76 ms.\n",
      "cifar100       train : [loss = 2.000, top1 = 46.39%], valid : [loss = 2.102, top1 = 44.17%], test : [loss = 2.100, top1 = 44.35%]\n",
      "ImageNet16-120 FLOP= 10.80 M, Params=0.323 MB, latency=9.51 ms.\n",
      "ImageNet16-120 train : [loss = 3.169, top1 = 23.59%], valid : [loss = 3.123, top1 = 24.93%], test : [loss = 3.175, top1 = 23.83%]\n",
      "tensor([[5.1758e-04, 1.7865e-03, 2.4616e-04, 6.3276e-04, 6.3033e-04],\n",
      "        [      -inf, 4.6908e-04, 9.2583e-04, 1.5301e-03, 1.0401e-04],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04, 5.4000e-04, 1.2266e-04],\n",
      "        [      -inf, 1.9962e-03, 9.0506e-04, 8.8039e-04, 9.5113e-05],\n",
      "        [      -inf, 1.9302e-03, 9.0115e-04, 1.6130e-03,       -inf],\n",
      "        [3.5560e-04, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|skip_connect~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=7143\n",
      "cifar10-valid  FLOP= 78.56 M, Params=0.559 MB, latency=15.21 ms.\n",
      "cifar10-valid  train : [loss = 0.180, top1 = 94.05%], valid : [loss = 0.540, top1 = 84.10%]\n",
      "cifar10        FLOP= 78.56 M, Params=0.559 MB, latency=15.07 ms.\n",
      "cifar10        train : [loss = 0.185, top1 = 93.82%], test  : [loss = 0.391, top1 = 87.62%]\n",
      "cifar100       FLOP= 78.57 M, Params=0.565 MB, latency=13.97 ms.\n",
      "cifar100       train : [loss = 1.184, top1 = 66.37%], valid : [loss = 1.495, top1 = 58.91%], test : [loss = 1.495, top1 = 58.53%]\n",
      "ImageNet16-120 FLOP= 19.65 M, Params=0.567 MB, latency=12.60 ms.\n",
      "ImageNet16-120 train : [loss = 2.595, top1 = 34.80%], valid : [loss = 2.658, top1 = 33.58%], test : [loss = 2.684, top1 = 32.70%]\n",
      "tensor([[5.1758e-04, 1.7865e-03, 2.4616e-04, 6.3276e-04, 6.3033e-04],\n",
      "        [      -inf, 4.6908e-04, 9.2583e-04, 1.5301e-03, 1.0401e-04],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04, 5.4000e-04, 1.2266e-04],\n",
      "        [      -inf, 1.9962e-03, 9.0506e-04, 8.8039e-04, 9.5113e-05],\n",
      "        [      -inf,       -inf, 9.0115e-04, 1.6130e-03,       -inf],\n",
      "        [3.5560e-04, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|skip_connect~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=7143\n",
      "cifar10-valid  FLOP= 78.56 M, Params=0.559 MB, latency=15.21 ms.\n",
      "cifar10-valid  train : [loss = 0.180, top1 = 94.05%], valid : [loss = 0.540, top1 = 84.10%]\n",
      "cifar10        FLOP= 78.56 M, Params=0.559 MB, latency=15.07 ms.\n",
      "cifar10        train : [loss = 0.185, top1 = 93.82%], test  : [loss = 0.391, top1 = 87.62%]\n",
      "cifar100       FLOP= 78.57 M, Params=0.565 MB, latency=13.97 ms.\n",
      "cifar100       train : [loss = 1.184, top1 = 66.37%], valid : [loss = 1.495, top1 = 58.91%], test : [loss = 1.495, top1 = 58.53%]\n",
      "ImageNet16-120 FLOP= 19.65 M, Params=0.567 MB, latency=12.60 ms.\n",
      "ImageNet16-120 train : [loss = 2.595, top1 = 34.80%], valid : [loss = 2.658, top1 = 33.58%], test : [loss = 2.684, top1 = 32.70%]\n",
      "tensor([[5.1758e-04, 1.7865e-03, 2.4616e-04, 6.3276e-04, 6.3033e-04],\n",
      "        [      -inf, 4.6908e-04, 9.2583e-04, 1.5301e-03,       -inf],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04, 5.4000e-04, 1.2266e-04],\n",
      "        [      -inf, 1.9962e-03, 9.0506e-04, 8.8039e-04, 9.5113e-05],\n",
      "        [      -inf,       -inf, 9.0115e-04, 1.6130e-03,       -inf],\n",
      "        [3.5560e-04, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|skip_connect~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=7143\n",
      "cifar10-valid  FLOP= 78.56 M, Params=0.559 MB, latency=15.21 ms.\n",
      "cifar10-valid  train : [loss = 0.180, top1 = 94.05%], valid : [loss = 0.540, top1 = 84.10%]\n",
      "cifar10        FLOP= 78.56 M, Params=0.559 MB, latency=15.07 ms.\n",
      "cifar10        train : [loss = 0.185, top1 = 93.82%], test  : [loss = 0.391, top1 = 87.62%]\n",
      "cifar100       FLOP= 78.57 M, Params=0.565 MB, latency=13.97 ms.\n",
      "cifar100       train : [loss = 1.184, top1 = 66.37%], valid : [loss = 1.495, top1 = 58.91%], test : [loss = 1.495, top1 = 58.53%]\n",
      "ImageNet16-120 FLOP= 19.65 M, Params=0.567 MB, latency=12.60 ms.\n",
      "ImageNet16-120 train : [loss = 2.595, top1 = 34.80%], valid : [loss = 2.658, top1 = 33.58%], test : [loss = 2.684, top1 = 32.70%]\n",
      "tensor([[5.1758e-04, 1.7865e-03,       -inf, 6.3276e-04, 6.3033e-04],\n",
      "        [      -inf, 4.6908e-04, 9.2583e-04, 1.5301e-03,       -inf],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04, 5.4000e-04, 1.2266e-04],\n",
      "        [      -inf, 1.9962e-03, 9.0506e-04, 8.8039e-04, 9.5113e-05],\n",
      "        [      -inf,       -inf, 9.0115e-04, 1.6130e-03,       -inf],\n",
      "        [3.5560e-04, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|skip_connect~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=7143\n",
      "cifar10-valid  FLOP= 78.56 M, Params=0.559 MB, latency=15.21 ms.\n",
      "cifar10-valid  train : [loss = 0.180, top1 = 94.05%], valid : [loss = 0.540, top1 = 84.10%]\n",
      "cifar10        FLOP= 78.56 M, Params=0.559 MB, latency=15.07 ms.\n",
      "cifar10        train : [loss = 0.185, top1 = 93.82%], test  : [loss = 0.391, top1 = 87.62%]\n",
      "cifar100       FLOP= 78.57 M, Params=0.565 MB, latency=13.97 ms.\n",
      "cifar100       train : [loss = 1.184, top1 = 66.37%], valid : [loss = 1.495, top1 = 58.91%], test : [loss = 1.495, top1 = 58.53%]\n",
      "ImageNet16-120 FLOP= 19.65 M, Params=0.567 MB, latency=12.60 ms.\n",
      "ImageNet16-120 train : [loss = 2.595, top1 = 34.80%], valid : [loss = 2.658, top1 = 33.58%], test : [loss = 2.684, top1 = 32.70%]\n",
      "tensor([[5.1758e-04, 1.7865e-03,       -inf, 6.3276e-04, 6.3033e-04],\n",
      "        [      -inf, 4.6908e-04, 9.2583e-04, 1.5301e-03,       -inf],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04, 5.4000e-04, 1.2266e-04],\n",
      "        [      -inf, 1.9962e-03, 9.0506e-04, 8.8039e-04, 9.5113e-05],\n",
      "        [      -inf,       -inf, 9.0115e-04, 1.6130e-03,       -inf],\n",
      "        [      -inf, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|skip_connect~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=7143\n",
      "cifar10-valid  FLOP= 78.56 M, Params=0.559 MB, latency=15.21 ms.\n",
      "cifar10-valid  train : [loss = 0.180, top1 = 94.05%], valid : [loss = 0.540, top1 = 84.10%]\n",
      "cifar10        FLOP= 78.56 M, Params=0.559 MB, latency=15.07 ms.\n",
      "cifar10        train : [loss = 0.185, top1 = 93.82%], test  : [loss = 0.391, top1 = 87.62%]\n",
      "cifar100       FLOP= 78.57 M, Params=0.565 MB, latency=13.97 ms.\n",
      "cifar100       train : [loss = 1.184, top1 = 66.37%], valid : [loss = 1.495, top1 = 58.91%], test : [loss = 1.495, top1 = 58.53%]\n",
      "ImageNet16-120 FLOP= 19.65 M, Params=0.567 MB, latency=12.60 ms.\n",
      "ImageNet16-120 train : [loss = 2.595, top1 = 34.80%], valid : [loss = 2.658, top1 = 33.58%], test : [loss = 2.684, top1 = 32.70%]\n",
      "tensor([[      -inf, 1.7865e-03,       -inf, 6.3276e-04, 6.3033e-04],\n",
      "        [      -inf, 4.6908e-04, 9.2583e-04, 1.5301e-03,       -inf],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04, 5.4000e-04, 1.2266e-04],\n",
      "        [      -inf, 1.9962e-03, 9.0506e-04, 8.8039e-04, 9.5113e-05],\n",
      "        [      -inf,       -inf, 9.0115e-04, 1.6130e-03,       -inf],\n",
      "        [      -inf, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|skip_connect~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=7143\n",
      "cifar10-valid  FLOP= 78.56 M, Params=0.559 MB, latency=15.21 ms.\n",
      "cifar10-valid  train : [loss = 0.180, top1 = 94.05%], valid : [loss = 0.540, top1 = 84.10%]\n",
      "cifar10        FLOP= 78.56 M, Params=0.559 MB, latency=15.07 ms.\n",
      "cifar10        train : [loss = 0.185, top1 = 93.82%], test  : [loss = 0.391, top1 = 87.62%]\n",
      "cifar100       FLOP= 78.57 M, Params=0.565 MB, latency=13.97 ms.\n",
      "cifar100       train : [loss = 1.184, top1 = 66.37%], valid : [loss = 1.495, top1 = 58.91%], test : [loss = 1.495, top1 = 58.53%]\n",
      "ImageNet16-120 FLOP= 19.65 M, Params=0.567 MB, latency=12.60 ms.\n",
      "ImageNet16-120 train : [loss = 2.595, top1 = 34.80%], valid : [loss = 2.658, top1 = 33.58%], test : [loss = 2.684, top1 = 32.70%]\n",
      "tensor([[      -inf, 1.7865e-03,       -inf, 6.3276e-04,       -inf],\n",
      "        [      -inf, 4.6908e-04, 9.2583e-04, 1.5301e-03,       -inf],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04, 5.4000e-04, 1.2266e-04],\n",
      "        [      -inf, 1.9962e-03, 9.0506e-04, 8.8039e-04, 9.5113e-05],\n",
      "        [      -inf,       -inf, 9.0115e-04, 1.6130e-03,       -inf],\n",
      "        [      -inf, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|skip_connect~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=7143\n",
      "cifar10-valid  FLOP= 78.56 M, Params=0.559 MB, latency=15.21 ms.\n",
      "cifar10-valid  train : [loss = 0.180, top1 = 94.05%], valid : [loss = 0.540, top1 = 84.10%]\n",
      "cifar10        FLOP= 78.56 M, Params=0.559 MB, latency=15.07 ms.\n",
      "cifar10        train : [loss = 0.185, top1 = 93.82%], test  : [loss = 0.391, top1 = 87.62%]\n",
      "cifar100       FLOP= 78.57 M, Params=0.565 MB, latency=13.97 ms.\n",
      "cifar100       train : [loss = 1.184, top1 = 66.37%], valid : [loss = 1.495, top1 = 58.91%], test : [loss = 1.495, top1 = 58.53%]\n",
      "ImageNet16-120 FLOP= 19.65 M, Params=0.567 MB, latency=12.60 ms.\n",
      "ImageNet16-120 train : [loss = 2.595, top1 = 34.80%], valid : [loss = 2.658, top1 = 33.58%], test : [loss = 2.684, top1 = 32.70%]\n",
      "tensor([[      -inf, 1.7865e-03,       -inf, 6.3276e-04,       -inf],\n",
      "        [      -inf, 4.6908e-04, 9.2583e-04, 1.5301e-03,       -inf],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04,       -inf, 1.2266e-04],\n",
      "        [      -inf, 1.9962e-03, 9.0506e-04, 8.8039e-04, 9.5113e-05],\n",
      "        [      -inf,       -inf, 9.0115e-04, 1.6130e-03,       -inf],\n",
      "        [      -inf, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|skip_connect~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=7143\n",
      "cifar10-valid  FLOP= 78.56 M, Params=0.559 MB, latency=15.21 ms.\n",
      "cifar10-valid  train : [loss = 0.180, top1 = 94.05%], valid : [loss = 0.540, top1 = 84.10%]\n",
      "cifar10        FLOP= 78.56 M, Params=0.559 MB, latency=15.07 ms.\n",
      "cifar10        train : [loss = 0.185, top1 = 93.82%], test  : [loss = 0.391, top1 = 87.62%]\n",
      "cifar100       FLOP= 78.57 M, Params=0.565 MB, latency=13.97 ms.\n",
      "cifar100       train : [loss = 1.184, top1 = 66.37%], valid : [loss = 1.495, top1 = 58.91%], test : [loss = 1.495, top1 = 58.53%]\n",
      "ImageNet16-120 FLOP= 19.65 M, Params=0.567 MB, latency=12.60 ms.\n",
      "ImageNet16-120 train : [loss = 2.595, top1 = 34.80%], valid : [loss = 2.658, top1 = 33.58%], test : [loss = 2.684, top1 = 32.70%]\n",
      "tensor([[      -inf, 1.7865e-03,       -inf, 6.3276e-04,       -inf],\n",
      "        [      -inf, 4.6908e-04,       -inf, 1.5301e-03,       -inf],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04,       -inf, 1.2266e-04],\n",
      "        [      -inf, 1.9962e-03, 9.0506e-04, 8.8039e-04, 9.5113e-05],\n",
      "        [      -inf,       -inf, 9.0115e-04, 1.6130e-03,       -inf],\n",
      "        [      -inf, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=15049\n",
      "cifar10-valid  FLOP=113.95 M, Params=0.802 MB, latency=18.59 ms.\n",
      "cifar10-valid  train : [loss = 0.002, top1 = 99.99%], valid : [loss = 0.430, top1 = 90.87%]\n",
      "cifar10        FLOP=113.95 M, Params=0.802 MB, latency=15.85 ms.\n",
      "cifar10        train : [loss = 0.002, top1 = 99.98%], test  : [loss = 0.295, top1 = 93.50%]\n",
      "cifar100       FLOP=113.96 M, Params=0.808 MB, latency=16.89 ms.\n",
      "cifar100       train : [loss = 0.056, top1 = 99.27%], valid : [loss = 1.372, top1 = 69.82%], test : [loss = 1.374, top1 = 70.56%]\n",
      "ImageNet16-120 FLOP= 28.50 M, Params=0.810 MB, latency=19.92 ms.\n",
      "ImageNet16-120 train : [loss = 1.512, top1 = 59.16%], valid : [loss = 2.244, top1 = 44.30%], test : [loss = 2.249, top1 = 43.63%]\n",
      "tensor([[      -inf,       -inf,       -inf, 6.3276e-04,       -inf],\n",
      "        [      -inf, 4.6908e-04,       -inf, 1.5301e-03,       -inf],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04,       -inf, 1.2266e-04],\n",
      "        [      -inf, 1.9962e-03, 9.0506e-04, 8.8039e-04, 9.5113e-05],\n",
      "        [      -inf,       -inf, 9.0115e-04, 1.6130e-03,       -inf],\n",
      "        [      -inf, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=15049\n",
      "cifar10-valid  FLOP=113.95 M, Params=0.802 MB, latency=18.59 ms.\n",
      "cifar10-valid  train : [loss = 0.002, top1 = 99.99%], valid : [loss = 0.430, top1 = 90.87%]\n",
      "cifar10        FLOP=113.95 M, Params=0.802 MB, latency=15.85 ms.\n",
      "cifar10        train : [loss = 0.002, top1 = 99.98%], test  : [loss = 0.295, top1 = 93.50%]\n",
      "cifar100       FLOP=113.96 M, Params=0.808 MB, latency=16.89 ms.\n",
      "cifar100       train : [loss = 0.056, top1 = 99.27%], valid : [loss = 1.372, top1 = 69.82%], test : [loss = 1.374, top1 = 70.56%]\n",
      "ImageNet16-120 FLOP= 28.50 M, Params=0.810 MB, latency=19.92 ms.\n",
      "ImageNet16-120 train : [loss = 1.512, top1 = 59.16%], valid : [loss = 2.244, top1 = 44.30%], test : [loss = 2.249, top1 = 43.63%]\n",
      "tensor([[      -inf,       -inf,       -inf, 6.3276e-04,       -inf],\n",
      "        [      -inf, 4.6908e-04,       -inf, 1.5301e-03,       -inf],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04,       -inf, 1.2266e-04],\n",
      "        [      -inf, 1.9962e-03, 9.0506e-04, 8.8039e-04,       -inf],\n",
      "        [      -inf,       -inf, 9.0115e-04, 1.6130e-03,       -inf],\n",
      "        [      -inf, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|skip_connect~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=15049\n",
      "cifar10-valid  FLOP=113.95 M, Params=0.802 MB, latency=18.59 ms.\n",
      "cifar10-valid  train : [loss = 0.002, top1 = 99.99%], valid : [loss = 0.430, top1 = 90.87%]\n",
      "cifar10        FLOP=113.95 M, Params=0.802 MB, latency=15.85 ms.\n",
      "cifar10        train : [loss = 0.002, top1 = 99.98%], test  : [loss = 0.295, top1 = 93.50%]\n",
      "cifar100       FLOP=113.96 M, Params=0.808 MB, latency=16.89 ms.\n",
      "cifar100       train : [loss = 0.056, top1 = 99.27%], valid : [loss = 1.372, top1 = 69.82%], test : [loss = 1.374, top1 = 70.56%]\n",
      "ImageNet16-120 FLOP= 28.50 M, Params=0.810 MB, latency=19.92 ms.\n",
      "ImageNet16-120 train : [loss = 1.512, top1 = 59.16%], valid : [loss = 2.244, top1 = 44.30%], test : [loss = 2.249, top1 = 43.63%]\n",
      "tensor([[      -inf,       -inf,       -inf, 6.3276e-04,       -inf],\n",
      "        [      -inf, 4.6908e-04,       -inf, 1.5301e-03,       -inf],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04,       -inf, 1.2266e-04],\n",
      "        [      -inf, 1.9962e-03, 9.0506e-04, 8.8039e-04,       -inf],\n",
      "        [      -inf,       -inf,       -inf, 1.6130e-03,       -inf],\n",
      "        [      -inf, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|nor_conv_1x1~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=12943\n",
      "cifar10-valid  FLOP=117.88 M, Params=0.830 MB, latency=17.61 ms.\n",
      "cifar10-valid  train : [loss = 0.003, top1 = 99.97%], valid : [loss = 0.505, top1 = 89.77%]\n",
      "cifar10        FLOP=117.88 M, Params=0.830 MB, latency=18.66 ms.\n",
      "cifar10        train : [loss = 0.005, top1 = 99.90%], test  : [loss = 0.325, top1 = 93.16%]\n",
      "cifar100       FLOP=117.89 M, Params=0.836 MB, latency=20.18 ms.\n",
      "cifar100       train : [loss = 0.080, top1 = 98.45%], valid : [loss = 1.393, top1 = 69.74%], test : [loss = 1.332, top1 = 70.34%]\n",
      "ImageNet16-120 FLOP= 29.48 M, Params=0.838 MB, latency=15.97 ms.\n",
      "ImageNet16-120 train : [loss = 1.672, top1 = 55.06%], valid : [loss = 2.176, top1 = 44.73%], test : [loss = 2.165, top1 = 44.90%]\n",
      "tensor([[      -inf,       -inf,       -inf, 6.3276e-04,       -inf],\n",
      "        [      -inf, 4.6908e-04,       -inf, 1.5301e-03,       -inf],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04,       -inf, 1.2266e-04],\n",
      "        [      -inf,       -inf, 9.0506e-04, 8.8039e-04,       -inf],\n",
      "        [      -inf,       -inf,       -inf, 1.6130e-03,       -inf],\n",
      "        [      -inf, 1.6399e-05, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|nor_conv_1x1~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=12943\n",
      "cifar10-valid  FLOP=117.88 M, Params=0.830 MB, latency=17.61 ms.\n",
      "cifar10-valid  train : [loss = 0.003, top1 = 99.97%], valid : [loss = 0.505, top1 = 89.77%]\n",
      "cifar10        FLOP=117.88 M, Params=0.830 MB, latency=18.66 ms.\n",
      "cifar10        train : [loss = 0.005, top1 = 99.90%], test  : [loss = 0.325, top1 = 93.16%]\n",
      "cifar100       FLOP=117.89 M, Params=0.836 MB, latency=20.18 ms.\n",
      "cifar100       train : [loss = 0.080, top1 = 98.45%], valid : [loss = 1.393, top1 = 69.74%], test : [loss = 1.332, top1 = 70.34%]\n",
      "ImageNet16-120 FLOP= 29.48 M, Params=0.838 MB, latency=15.97 ms.\n",
      "ImageNet16-120 train : [loss = 1.672, top1 = 55.06%], valid : [loss = 2.176, top1 = 44.73%], test : [loss = 2.165, top1 = 44.90%]\n",
      "tensor([[      -inf,       -inf,       -inf, 6.3276e-04,       -inf],\n",
      "        [      -inf, 4.6908e-04,       -inf, 1.5301e-03,       -inf],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04,       -inf, 1.2266e-04],\n",
      "        [      -inf,       -inf, 9.0506e-04, 8.8039e-04,       -inf],\n",
      "        [      -inf,       -inf,       -inf, 1.6130e-03,       -inf],\n",
      "        [      -inf,       -inf, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|nor_conv_1x1~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=12943\n",
      "cifar10-valid  FLOP=117.88 M, Params=0.830 MB, latency=17.61 ms.\n",
      "cifar10-valid  train : [loss = 0.003, top1 = 99.97%], valid : [loss = 0.505, top1 = 89.77%]\n",
      "cifar10        FLOP=117.88 M, Params=0.830 MB, latency=18.66 ms.\n",
      "cifar10        train : [loss = 0.005, top1 = 99.90%], test  : [loss = 0.325, top1 = 93.16%]\n",
      "cifar100       FLOP=117.89 M, Params=0.836 MB, latency=20.18 ms.\n",
      "cifar100       train : [loss = 0.080, top1 = 98.45%], valid : [loss = 1.393, top1 = 69.74%], test : [loss = 1.332, top1 = 70.34%]\n",
      "ImageNet16-120 FLOP= 29.48 M, Params=0.838 MB, latency=15.97 ms.\n",
      "ImageNet16-120 train : [loss = 1.672, top1 = 55.06%], valid : [loss = 2.176, top1 = 44.73%], test : [loss = 2.165, top1 = 44.90%]\n",
      "tensor([[      -inf,       -inf,       -inf, 6.3276e-04,       -inf],\n",
      "        [      -inf,       -inf,       -inf, 1.5301e-03,       -inf],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04,       -inf, 1.2266e-04],\n",
      "        [      -inf,       -inf, 9.0506e-04, 8.8039e-04,       -inf],\n",
      "        [      -inf,       -inf,       -inf, 1.6130e-03,       -inf],\n",
      "        [      -inf,       -inf, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|nor_conv_1x1~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=12943\n",
      "cifar10-valid  FLOP=117.88 M, Params=0.830 MB, latency=17.61 ms.\n",
      "cifar10-valid  train : [loss = 0.003, top1 = 99.97%], valid : [loss = 0.505, top1 = 89.77%]\n",
      "cifar10        FLOP=117.88 M, Params=0.830 MB, latency=18.66 ms.\n",
      "cifar10        train : [loss = 0.005, top1 = 99.90%], test  : [loss = 0.325, top1 = 93.16%]\n",
      "cifar100       FLOP=117.89 M, Params=0.836 MB, latency=20.18 ms.\n",
      "cifar100       train : [loss = 0.080, top1 = 98.45%], valid : [loss = 1.393, top1 = 69.74%], test : [loss = 1.332, top1 = 70.34%]\n",
      "ImageNet16-120 FLOP= 29.48 M, Params=0.838 MB, latency=15.97 ms.\n",
      "ImageNet16-120 train : [loss = 1.672, top1 = 55.06%], valid : [loss = 2.176, top1 = 44.73%], test : [loss = 2.165, top1 = 44.90%]\n",
      "tensor([[      -inf,       -inf,       -inf, 6.3276e-04,       -inf],\n",
      "        [      -inf,       -inf,       -inf, 1.5301e-03,       -inf],\n",
      "        [2.9429e-04, 8.8370e-04, 7.1474e-04,       -inf, 1.2266e-04],\n",
      "        [      -inf,       -inf, 9.0506e-04,       -inf,       -inf],\n",
      "        [      -inf,       -inf,       -inf, 1.6130e-03,       -inf],\n",
      "        [      -inf,       -inf, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|nor_conv_1x1~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=12943\n",
      "cifar10-valid  FLOP=117.88 M, Params=0.830 MB, latency=17.61 ms.\n",
      "cifar10-valid  train : [loss = 0.003, top1 = 99.97%], valid : [loss = 0.505, top1 = 89.77%]\n",
      "cifar10        FLOP=117.88 M, Params=0.830 MB, latency=18.66 ms.\n",
      "cifar10        train : [loss = 0.005, top1 = 99.90%], test  : [loss = 0.325, top1 = 93.16%]\n",
      "cifar100       FLOP=117.89 M, Params=0.836 MB, latency=20.18 ms.\n",
      "cifar100       train : [loss = 0.080, top1 = 98.45%], valid : [loss = 1.393, top1 = 69.74%], test : [loss = 1.332, top1 = 70.34%]\n",
      "ImageNet16-120 FLOP= 29.48 M, Params=0.838 MB, latency=15.97 ms.\n",
      "ImageNet16-120 train : [loss = 1.672, top1 = 55.06%], valid : [loss = 2.176, top1 = 44.73%], test : [loss = 2.165, top1 = 44.90%]\n",
      "tensor([[      -inf,       -inf,       -inf, 6.3276e-04,       -inf],\n",
      "        [      -inf,       -inf,       -inf, 1.5301e-03,       -inf],\n",
      "        [2.9429e-04, 8.8370e-04,       -inf,       -inf, 1.2266e-04],\n",
      "        [      -inf,       -inf, 9.0506e-04,       -inf,       -inf],\n",
      "        [      -inf,       -inf,       -inf, 1.6130e-03,       -inf],\n",
      "        [      -inf,       -inf, 4.5186e-04, 7.4616e-06, 1.1256e-03]],\n",
      "       device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|nor_conv_1x1~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=12943\n",
      "cifar10-valid  FLOP=117.88 M, Params=0.830 MB, latency=17.61 ms.\n",
      "cifar10-valid  train : [loss = 0.003, top1 = 99.97%], valid : [loss = 0.505, top1 = 89.77%]\n",
      "cifar10        FLOP=117.88 M, Params=0.830 MB, latency=18.66 ms.\n",
      "cifar10        train : [loss = 0.005, top1 = 99.90%], test  : [loss = 0.325, top1 = 93.16%]\n",
      "cifar100       FLOP=117.89 M, Params=0.836 MB, latency=20.18 ms.\n",
      "cifar100       train : [loss = 0.080, top1 = 98.45%], valid : [loss = 1.393, top1 = 69.74%], test : [loss = 1.332, top1 = 70.34%]\n",
      "ImageNet16-120 FLOP= 29.48 M, Params=0.838 MB, latency=15.97 ms.\n",
      "ImageNet16-120 train : [loss = 1.672, top1 = 55.06%], valid : [loss = 2.176, top1 = 44.73%], test : [loss = 2.165, top1 = 44.90%]\n",
      "tensor([[  -inf,   -inf,   -inf, 0.0006,   -inf],\n",
      "        [  -inf,   -inf,   -inf, 0.0015,   -inf],\n",
      "        [0.0003, 0.0009,   -inf,   -inf, 0.0001],\n",
      "        [  -inf,   -inf, 0.0009,   -inf,   -inf],\n",
      "        [  -inf,   -inf,   -inf, 0.0016,   -inf],\n",
      "        [  -inf,   -inf, 0.0005,   -inf, 0.0011]], device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|nor_conv_1x1~0|nor_conv_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=12943\n",
      "cifar10-valid  FLOP=117.88 M, Params=0.830 MB, latency=17.61 ms.\n",
      "cifar10-valid  train : [loss = 0.003, top1 = 99.97%], valid : [loss = 0.505, top1 = 89.77%]\n",
      "cifar10        FLOP=117.88 M, Params=0.830 MB, latency=18.66 ms.\n",
      "cifar10        train : [loss = 0.005, top1 = 99.90%], test  : [loss = 0.325, top1 = 93.16%]\n",
      "cifar100       FLOP=117.89 M, Params=0.836 MB, latency=20.18 ms.\n",
      "cifar100       train : [loss = 0.080, top1 = 98.45%], valid : [loss = 1.393, top1 = 69.74%], test : [loss = 1.332, top1 = 70.34%]\n",
      "ImageNet16-120 FLOP= 29.48 M, Params=0.838 MB, latency=15.97 ms.\n",
      "ImageNet16-120 train : [loss = 1.672, top1 = 55.06%], valid : [loss = 2.176, top1 = 44.73%], test : [loss = 2.165, top1 = 44.90%]\n",
      "tensor([[  -inf,   -inf,   -inf, 0.0006,   -inf],\n",
      "        [  -inf,   -inf,   -inf, 0.0015,   -inf],\n",
      "        [  -inf, 0.0009,   -inf,   -inf, 0.0001],\n",
      "        [  -inf,   -inf, 0.0009,   -inf,   -inf],\n",
      "        [  -inf,   -inf,   -inf, 0.0016,   -inf],\n",
      "        [  -inf,   -inf, 0.0005,   -inf, 0.0011]], device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|nor_conv_1x1~0|nor_conv_3x3~1|nor_conv_1x1~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=9648\n",
      "cifar10-valid  FLOP=121.82 M, Params=0.858 MB, latency=20.16 ms.\n",
      "cifar10-valid  train : [loss = 0.003, top1 = 99.96%], valid : [loss = 0.487, top1 = 90.30%]\n",
      "cifar10        FLOP=121.82 M, Params=0.858 MB, latency=21.33 ms.\n",
      "cifar10        train : [loss = 0.004, top1 = 99.93%], test  : [loss = 0.307, top1 = 93.45%]\n",
      "cifar100       FLOP=121.82 M, Params=0.864 MB, latency=22.70 ms.\n",
      "cifar100       train : [loss = 0.060, top1 = 99.20%], valid : [loss = 1.241, top1 = 71.66%], test : [loss = 1.210, top1 = 72.40%]\n",
      "ImageNet16-120 FLOP= 30.46 M, Params=0.866 MB, latency=19.32 ms.\n",
      "ImageNet16-120 train : [loss = 1.536, top1 = 58.38%], valid : [loss = 2.128, top1 = 44.97%], test : [loss = 2.134, top1 = 45.37%]\n",
      "tensor([[  -inf,   -inf,   -inf, 0.0006,   -inf],\n",
      "        [  -inf,   -inf,   -inf, 0.0015,   -inf],\n",
      "        [  -inf, 0.0009,   -inf,   -inf, 0.0001],\n",
      "        [  -inf,   -inf, 0.0009,   -inf,   -inf],\n",
      "        [  -inf,   -inf,   -inf, 0.0016,   -inf],\n",
      "        [  -inf,   -inf, 0.0005,   -inf,   -inf]], device='cuda:0')\n",
      "---------------------------------------------\n",
      "random\n",
      "|nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|nor_conv_1x1~0|nor_conv_3x3~1|nor_conv_1x1~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=9648\n",
      "cifar10-valid  FLOP=121.82 M, Params=0.858 MB, latency=20.16 ms.\n",
      "cifar10-valid  train : [loss = 0.003, top1 = 99.96%], valid : [loss = 0.487, top1 = 90.30%]\n",
      "cifar10        FLOP=121.82 M, Params=0.858 MB, latency=21.33 ms.\n",
      "cifar10        train : [loss = 0.004, top1 = 99.93%], test  : [loss = 0.307, top1 = 93.45%]\n",
      "cifar100       FLOP=121.82 M, Params=0.864 MB, latency=22.70 ms.\n",
      "cifar100       train : [loss = 0.060, top1 = 99.20%], valid : [loss = 1.241, top1 = 71.66%], test : [loss = 1.210, top1 = 72.40%]\n",
      "ImageNet16-120 FLOP= 30.46 M, Params=0.866 MB, latency=19.32 ms.\n",
      "ImageNet16-120 train : [loss = 1.536, top1 = 58.38%], valid : [loss = 2.128, top1 = 44.97%], test : [loss = 2.134, top1 = 45.37%]\n",
      "tensor([[  -inf,   -inf,   -inf, 0.0006,   -inf],\n",
      "        [  -inf,   -inf,   -inf, 0.0015,   -inf],\n",
      "        [  -inf, 0.0009,   -inf,   -inf,   -inf],\n",
      "        [  -inf,   -inf, 0.0009,   -inf,   -inf],\n",
      "        [  -inf,   -inf,   -inf, 0.0016,   -inf],\n",
      "        [  -inf,   -inf, 0.0005,   -inf,   -inf]], device='cuda:0')\n",
      "---------------------------------------------\n",
      "kaishi-----------------------------------\n",
      "|nor_conv_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|nor_conv_1x1~0|nor_conv_3x3~1|nor_conv_1x1~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=9648\n",
      "cifar10-valid  FLOP=121.82 M, Params=0.858 MB, latency=20.16 ms.\n",
      "cifar10-valid  train : [loss = 0.003, top1 = 99.96%], valid : [loss = 0.487, top1 = 90.30%]\n",
      "cifar10        FLOP=121.82 M, Params=0.858 MB, latency=21.33 ms.\n",
      "cifar10        train : [loss = 0.004, top1 = 99.93%], test  : [loss = 0.307, top1 = 93.45%]\n",
      "cifar100       FLOP=121.82 M, Params=0.864 MB, latency=22.70 ms.\n",
      "cifar100       train : [loss = 0.060, top1 = 99.20%], valid : [loss = 1.241, top1 = 71.66%], test : [loss = 1.210, top1 = 72.40%]\n",
      "ImageNet16-120 FLOP= 30.46 M, Params=0.866 MB, latency=19.32 ms.\n",
      "ImageNet16-120 train : [loss = 1.536, top1 = 58.38%], valid : [loss = 2.128, top1 = 44.97%], test : [loss = 2.134, top1 = 45.37%]\n",
      "Pruning cost 4.8 s.\n",
      "Pruning finished\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "args.save_dir= './00FreeDARTS_SynFlow_seed_'+str(args.rand_seed)+args.save_dir\n",
    "\n",
    "xargs=args\n",
    "assert torch.cuda.is_available(), 'CUDA is not available.'\n",
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.enabled   = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.set_num_threads( xargs.workers )\n",
    "prepare_seed(xargs.rand_seed)\n",
    "logger = prepare_logger(args)\n",
    "\n",
    "train_data, valid_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "if xargs.dataset == 'cifar10' or xargs.dataset == 'cifar100':\n",
    "    split_Fpath = '/home/cwang4/Data/Projects/AngleNAS-miao/NAS_Bench201/configs/nas-benchmark/cifar-split.txt'\n",
    "    cifar_split = load_config(split_Fpath, None, None)\n",
    "    train_split, valid_split = cifar_split.train, cifar_split.valid\n",
    "    logger.log('Load split file from {:}'.format(split_Fpath))\n",
    "elif xargs.dataset.startswith('ImageNet16'):\n",
    "    split_Fpath = 'configs/nas-benchmark/{:}-split.txt'.format(xargs.dataset)\n",
    "    imagenet16_split = load_config(split_Fpath, None, None)\n",
    "    train_split, valid_split = imagenet16_split.train, imagenet16_split.valid\n",
    "    logger.log('Load split file from {:}'.format(split_Fpath))\n",
    "else:\n",
    "    raise ValueError('invalid dataset : {:}'.format(xargs.dataset))\n",
    "config_path = '/home/cwang4/Data/Projects/AngleNAS-miao/NAS_Bench201/configs/nas-benchmark/algos/DARTS.config'\n",
    "config = load_config(config_path, {'class_num': class_num, 'xshape': xshape}, logger)\n",
    "# To split data\n",
    "train_data_v2 = deepcopy(train_data)\n",
    "train_data_v2.transform = valid_data.transform\n",
    "valid_data    = train_data_v2\n",
    "search_data   = SearchDataset(xargs.dataset, train_data, train_split, valid_split)\n",
    "# data loader\n",
    "search_loader = torch.utils.data.DataLoader(search_data, batch_size=config.batch_size, shuffle=True , num_workers=xargs.workers, pin_memory=True)\n",
    "valid_loader  = torch.utils.data.DataLoader(valid_data, batch_size=config.batch_size, sampler=torch.utils.data.sampler.SubsetRandomSampler(valid_split), num_workers=xargs.workers, pin_memory=True)\n",
    "logger.log('||||||| {:10s} ||||||| Search-Loader-Num={:}, Valid-Loader-Num={:}, batch size={:}'.format(xargs.dataset, len(search_loader), len(valid_loader), config.batch_size))\n",
    "logger.log('||||||| {:10s} ||||||| Config={:}'.format(xargs.dataset, config))\n",
    "\n",
    "search_space = get_search_spaces('cell', xargs.search_space_name)\n",
    "model_config = dict2config({'name': 'DARTS-V1', 'C': xargs.channel, 'N': xargs.num_cells,\n",
    "                              'max_nodes': xargs.max_nodes, 'num_classes': class_num,\n",
    "                              'space'    : search_space}, None)\n",
    "search_model = get_cell_based_tiny_net(model_config)\n",
    "# logger.log('search-model :\\n{:}'.format(search_model))\n",
    "\n",
    "\n",
    "w_optimizer, w_scheduler, criterion = get_optim_scheduler(search_model.get_weights(), config)\n",
    "a_optimizer = torch.optim.Adam(search_model.get_alphas(), lr=xargs.arch_learning_rate, betas=(0.5, 0.999), weight_decay=xargs.arch_weight_decay)\n",
    "\n",
    "\n",
    "logger.log('w-optimizer : {:}'.format(w_optimizer))\n",
    "logger.log('a-optimizer : {:}'.format(a_optimizer))\n",
    "logger.log('w-scheduler : {:}'.format(w_scheduler))\n",
    "logger.log('criterion   : {:}'.format(criterion))\n",
    "flop, param  = get_model_infos(search_model, xshape)\n",
    "#logger.log('{:}'.format(search_model))\n",
    "logger.log('FLOP = {:.2f} M, Params = {:.2f} MB'.format(flop, param))\n",
    "if xargs.arch_nas_dataset is None:\n",
    "    api = None\n",
    "else:\n",
    "    api = API(xargs.arch_nas_dataset)\n",
    "logger.log('{:} create API = {:} done'.format(time_string(), api))\n",
    "\n",
    "last_info, model_base_path, model_best_path = logger.path('info'), logger.path('model'), logger.path('best')\n",
    "network, criterion = torch.nn.DataParallel(search_model).cuda(), criterion.cuda()\n",
    "\n",
    "\n",
    "  # start training\n",
    "start_time, search_time, epoch_time, total_epoch = time.time(), AverageMeter(), AverageMeter(), config.epochs + config.warmup\n",
    "epoch=0\n",
    "w_scheduler.update(0, 0.0)\n",
    "\n",
    "arch_pruned = pruning_func(search_loader, network, criterion, w_scheduler, w_optimizer, a_optimizer, xargs.print_freq, logger)\n",
    "search_time.update(time.time() - start_time)\n",
    "logger.log('{:}'.format(api.query_by_arch(network.module.genotype_prune(arch_pruned))))   \n",
    "logger.log('Pruning cost {:.1f} s.'.format(search_time.sum))\n",
    "logger.log('Pruning finished')\n",
    "logger.log('-----------------------------------------------------------------')\n",
    "# if api is not None: logger.log('{:}'.format(api.query_by_arch(network.module.genotype_prune(arch_pruned)))) \n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
